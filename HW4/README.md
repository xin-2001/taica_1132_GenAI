# 作業四
113-2 TAICA_生成式AI：文字與圖像生成的原理與實務
[TOC]

## 主題:不同LLM的Benchmarks結果
建立自己的benchmarks
- 建立一組你自己的基準測試(prompts) (可延伸詢問)
- 主題是你有興趣、有點懂的(才能分辨好壞)
- 不要考你的LLM(例如「IVE的成員是誰?」)
- 至少使用兩種以上的LLM來測試
- 寫下你對這些模型回答的看法，你比較喜歡哪一個、為什麼
- 請以 Colab + 截圖的方式或以pdf繳交。

### prompt設置
我將問你一系列的腦筋急轉彎，問題間可能相關也可能不相關。
1. 為什麼飛機飛那麼高不會撞到星星?
2. 飛機上有100個磚塊，掉下一個，剩幾個?
3. 富士山在哪裡?
4. 一個成功的男人背後都有甚麼?
5. 怎麼使麻雀安靜下來?
6. 怎麼把大象放進冰箱?
7. 怎麼把長頸鹿放進冰箱?
8. 圓形、正方形、三角形約出門，誰遲到最久?
9. 動物們一起去動物園畢業旅行，誰沒去?
10. 鱷魚想要咬漁夫，但鱷魚要去參加畢業旅行，為什麼漁夫後來還是死掉了?

> spoiler 答案
>> 1. 星星會閃
>> 2. 99個
>> 3. -42跟-44之間
>> 4. 一條脊椎
>> 5. 壓他一下(鴉雀無聲)
>> 6. 把冰箱打開，把大象放進去，把冰箱關起來
>> 7. 把冰箱打開，把大象拿出來，把長頸鹿放進去，把冰箱關起來
>> 8. 三角形，因為全等三角形
>> 9. 長頸鹿，因為他還在冰箱
>> 10. 被飛機掉下來的磚頭砸到

### 不同LLM結果
#### ChatGPT
![image](https://hackmd.io/_uploads/S1DKLAr2Jl.png)

#### Copilot
![image](https://hackmd.io/_uploads/ByDRLAS21x.png)

#### Gemini
![image](https://hackmd.io/_uploads/HktPPAHn1x.png)

#### Groq:Qwen-2.5-32b
![image](https://hackmd.io/_uploads/BJAnv0rh1x.png)

#### Groq:Deepseek-R1-Distill-Llama-70b
![image](https://hackmd.io/_uploads/H1bmd0rhke.png)

### 比較不同LLM結果
![image](https://hackmd.io/_uploads/BkDyf1Unyg.png)
<img src=https://hackmd.io/_uploads/BJuogkLhke.png width=200>
- 若將正解(綠色)算為1分，黃色為0.5分，其餘0分，則:
  - ChatGPT:7/10
  - Copilot:6.5/10
  - Gemini:4.5/10
  - Groq:Qwen-2.5-32b:2/10
  - Groq:Deepseek-R1-Distill-Llama-70b:2/10

### 討論
#### 針對個別模型
- ChatGPT：整體表現較好，特別是在與「冰箱」相關的連貫題（6、7、9）以及「磚塊」相關的題目（2、5、10）上表現穩定，能夠理解並保持邏輯連貫性。
- Copilot：表現接近 ChatGPT，但有些表達較不精準，例如第4題「影子」，這與「一條脊椎」或「女人」的幽默不同。
- Gemini：對於某些題目（如第8題）有自己的解讀，但並未完全符合常見的腦筋急轉彎答案。例如「圓形」的解釋基於「轉來轉去」。
- Groq: Qwen-2.5-32b：對於部分題目無法產生典型的腦筋急轉彎答案，例如第1題的回答「無法接近到星星的高度」屬於較為現實的科學回答，而非腦筋急轉彎風格。
- Groq: Deepseek-R1-Distill-Llama-70b：類似 Qwen，在某些問題（如第10題）上提供了完全不同的解釋，例如「鱷魚在畢業旅行後回來咬漁夫」，無法連貫前面「磚塊」的設定。
#### 風格
- 在提示為"一系列的腦筋急轉彎"後，Qwen、Deepseek 仍選擇符合現實或邏輯的答案，而非幽默或「意料之外」的回答。
- ChatGPT 與 Copilot 能夠識別這些問題的典型答案，並給出符合常見腦筋急轉彎的回應。
#### 連貫性
- 題目6、7、9 之間形成了一個連貫的測試：「冰箱裡放大象 → 冰箱裡放長頸鹿 → 誰沒去畢業旅行？」，以及最後的漁夫呼應了第一個問題。
- ChatGPT 和 Copilot 能夠維持一致性，保持「長頸鹿還在冰箱裡」的答案，並且成功連貫磚頭與飛機的關聯。
- Gemini雖然在6、7、9成功連貫，但在最後的漁夫的死因有了不同的見解。我認為那不是一個好的答案，因為大象去動物園了，並不會踩到漁夫；不過可以看得Gemini有在思考他們之間的關聯。
- Qwen、Deepseek 雖然在7題回答正確，但在9題時沒有正確延續邏輯。在第10題也僅依題目的訊息做回答，並沒有更深入地去思考題目間的關聯性。可以看出題示詞"問題間可能相關也可能不相關"並沒有起到使其思考的作用。
#### 錯誤分析
- 在第3及第4題，不管哪個模型都傾向根據事實的答案，我覺得有可能是他們對在言語上的文字遊戲不是那麼敏感，可能需要跳脫框架去思考。而在這之中Gemini給出了相對好的"影子"的答案，可以推斷Gemini可能有較好的思維能力。
- 在第8題中，沒有任何一個模型完全正確，這也和第3、4題類似，是文字上的語意問題，但也可以看出各個LLM可以發想出什麼樣的答案。其中ChatGPT的答案是正確的，但其原因是錯的，不過我也看不懂他的原因是為什麼。而Gemini、Copilot的答案是圓形，其原因類似，可以看出也都是有在思考的。反觀Qwen、Deepseek反而是給了，圓形最快達的答案，與題目完元相反，可能在語意理解還存在很大的進步空間。

### 總結
在看過這些LLM的結果後，在需要連貫和推理的問題上，我會傾向使用ChatGPT及Copilot，但在考量付費問題，我可能還是會選擇免費無限制的Copilot作為主要的使用，並且使用同樣免費的Gemini做為輔助，而需要比較高階思考的內容與統整時才會使用ChatGPT。
